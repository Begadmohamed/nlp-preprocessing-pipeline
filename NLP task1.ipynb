{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c937f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ghane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ghane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ghane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ghane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ghane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b3a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Package abc>\n",
      "<Package alpino>\n",
      "<Package averaged_perceptron_tagger>\n",
      "<Package averaged_perceptron_tagger_eng>\n",
      "<Package averaged_perceptron_tagger_ru>\n",
      "<Package averaged_perceptron_tagger_rus>\n",
      "<Package basque_grammars>\n",
      "<Package bcp47>\n",
      "<Package biocreative_ppi>\n",
      "<Package bllip_wsj_no_aux>\n",
      "<Package book_grammars>\n",
      "<Package brown>\n",
      "<Package brown_tei>\n",
      "<Package cess_cat>\n",
      "<Package cess_esp>\n",
      "<Package chat80>\n",
      "<Package city_database>\n",
      "<Package cmudict>\n",
      "<Package comparative_sentences>\n",
      "<Package comtrans>\n",
      "<Package conll2000>\n",
      "<Package conll2002>\n",
      "<Package conll2007>\n",
      "<Package crubadan>\n",
      "<Package dependency_treebank>\n",
      "<Package dolch>\n",
      "<Package english_wordnet>\n",
      "<Package europarl_raw>\n",
      "<Package extended_omw>\n",
      "<Package floresta>\n",
      "<Package framenet_v15>\n",
      "<Package framenet_v17>\n",
      "<Package gazetteers>\n",
      "<Package genesis>\n",
      "<Package gutenberg>\n",
      "<Package ieer>\n",
      "<Package inaugural>\n",
      "<Package indian>\n",
      "<Package jeita>\n",
      "<Package kimmo>\n",
      "<Package knbc>\n",
      "<Package large_grammars>\n",
      "<Package lin_thesaurus>\n",
      "<Package mac_morpho>\n",
      "<Package machado>\n",
      "<Package masc_tagged>\n",
      "<Package maxent_ne_chunker>\n",
      "<Package maxent_ne_chunker_tab>\n",
      "<Package maxent_treebank_pos_tagger>\n",
      "<Package maxent_treebank_pos_tagger_tab>\n",
      "<Package mock_corpus>\n",
      "<Package moses_sample>\n",
      "<Package movie_reviews>\n",
      "<Package mte_teip5>\n",
      "<Package mwa_ppdb>\n",
      "<Package names>\n",
      "<Package nombank.1.0>\n",
      "<Package nonbreaking_prefixes>\n",
      "<Package nps_chat>\n",
      "<Package omw>\n",
      "<Package omw-1.4>\n",
      "<Package opinion_lexicon>\n",
      "<Package panlex_swadesh>\n",
      "<Package paradigms>\n",
      "<Package pe08>\n",
      "<Package perluniprops>\n",
      "<Package pil>\n",
      "<Package pl196x>\n",
      "<Package porter_test>\n",
      "<Package ppattach>\n",
      "<Package problem_reports>\n",
      "<Package product_reviews_1>\n",
      "<Package product_reviews_2>\n",
      "<Package propbank>\n",
      "<Package pros_cons>\n",
      "<Package ptb>\n",
      "<Package punkt>\n",
      "<Package punkt_tab>\n",
      "<Package qc>\n",
      "<Package reuters>\n",
      "<Package rslp>\n",
      "<Package rte>\n",
      "<Package sample_grammars>\n",
      "<Package semcor>\n",
      "<Package senseval>\n",
      "<Package sentence_polarity>\n",
      "<Package sentiwordnet>\n",
      "<Package shakespeare>\n",
      "<Package sinica_treebank>\n",
      "<Package smultron>\n",
      "<Package snowball_data>\n",
      "<Package spanish_grammars>\n",
      "<Package state_union>\n",
      "<Package stopwords>\n",
      "<Package subjectivity>\n",
      "<Package swadesh>\n",
      "<Package switchboard>\n",
      "<Package tagsets>\n",
      "<Package tagsets_json>\n",
      "<Package timit>\n",
      "<Package toolbox>\n",
      "<Package treebank>\n",
      "<Package twitter_samples>\n",
      "<Package udhr>\n",
      "<Package udhr2>\n",
      "<Package unicode_samples>\n",
      "<Package universal_tagset>\n",
      "<Package universal_treebanks_v20>\n",
      "<Package vader_lexicon>\n",
      "<Package verbnet>\n",
      "<Package verbnet3>\n",
      "<Package webtext>\n",
      "<Package wmt15_eval>\n",
      "<Package word2vec_sample>\n",
      "<Package wordnet>\n",
      "<Package wordnet2021>\n",
      "<Package wordnet2022>\n",
      "<Package wordnet31>\n",
      "<Package wordnet_ic>\n",
      "<Package words>\n",
      "<Package ycoe>\n"
     ]
    }
   ],
   "source": [
    "from nltk import downloader\n",
    "d = downloader.Downloader()\n",
    "for pkg in d.packages():\n",
    "    print(pkg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccc0c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Download the 'punkt' resource (tokenizer)\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7d38ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7abf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
    "    \"I hated the film, it was the worst I have ever seen\",\n",
    "    \"The storyline was boring but the acting was brilliant\",\n",
    "    \"An amazing movie with a great plot and incredible performances\",\n",
    "    \"Egypt movie, I regret wasting my time on it\",\n",
    "    \"The actors did a great job but the story lacked depth\",\n",
    "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
    "    \"This film was just okay, not too bad but not great either\",\n",
    "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
    "    \"The movie was disappointing, it did not live up to the hype\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3301ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Tokenization\n",
    "## Sentence-->paragraphs\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ddddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: The movie was fantastic and I loved every part of it about Egypt\n",
      "\n",
      "\n",
      "Sentence 2: I hated the film, it was the worst I have ever seen\n",
      "\n",
      "\n",
      "Sentence 3: The storyline was boring but the acting was brilliant\n",
      "\n",
      "\n",
      "Sentence 4: An amazing movie with a great plot and incredible performances\n",
      "\n",
      "\n",
      "Sentence 5: Egypt movie, I regret wasting my time on it\n",
      "\n",
      "\n",
      "Sentence 6: The actors did a great job but the story lacked depth\n",
      "\n",
      "\n",
      "Sentence 7: One of the best films I have seen in a long time, highly recommend it\n",
      "\n",
      "\n",
      "Sentence 8: This film was just okay, not too bad but not great either\n",
      "\n",
      "\n",
      "Sentence 9: Absolutely loved the movie, fantastic plot and wonderful cast\n",
      "\n",
      "\n",
      "Sentence 10: The movie was disappointing, it did not live up to the hype\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num=0 \n",
    "for sentence in text_data:\n",
    "    num+=1\n",
    "    print(f\"Sentence {num}: {sentence}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9291f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Paragraph-->words\n",
    "## sentence--->words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fede881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'movie',\n",
       " 'was',\n",
       " 'fantastic',\n",
       " 'and',\n",
       " 'I',\n",
       " 'loved',\n",
       " 'every',\n",
       " 'part',\n",
       " 'of',\n",
       " 'it',\n",
       " 'about',\n",
       " 'Egypt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text_data[0])  # Tokenize the first sentence in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "674d867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'movie', 'was', 'fantastic', 'and', 'I', 'loved', 'every', 'part', 'of', 'it', 'about', 'Egypt', 'I', 'hated', 'the', 'film', ',', 'it', 'was', 'the', 'worst', 'I', 'have', 'ever', 'seen', 'The', 'storyline', 'was', 'boring', 'but', 'the', 'acting', 'was', 'brilliant', 'An', 'amazing', 'movie', 'with', 'a', 'great', 'plot', 'and', 'incredible', 'performances', 'Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it', 'The', 'actors', 'did', 'a', 'great', 'job', 'but', 'the', 'story', 'lacked', 'depth', 'One', 'of', 'the', 'best', 'films', 'I', 'have', 'seen', 'in', 'a', 'long', 'time', ',', 'highly', 'recommend', 'it', 'This', 'film', 'was', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'great', 'either', 'Absolutely', 'loved', 'the', 'movie', ',', 'fantastic', 'plot', 'and', 'wonderful', 'cast', 'The', 'movie', 'was', 'disappointing', ',', 'it', 'did', 'not', 'live', 'up', 'to', 'the', 'hype']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in text_data:\n",
    "    words.extend(word_tokenize(sentence))\n",
    "print(words)  # Print all tokenized words from all sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead37af0",
   "metadata": {},
   "source": [
    "## Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c38aeff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words len :  118\n",
      "lemmatized_words len :  56\n",
      "movie fantastic loved every part egypt hated film worst ever seen storyline boring acting brilliant amazing movie great plot incredible performance egypt movie regret wasting time actor great job story lacked depth one best film seen long time highly recommend film okay bad great either absolutely loved movie fantastic plot wonderful cast movie disappointing live hype\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "text_data = [\n",
    "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
    "    \"I hated the film, it was the worst I have ever seen\",\n",
    "    \"The storyline was boring but the acting was brilliant\",\n",
    "    \"An amazing movie with a great plot and incredible performances\",\n",
    "    \"Egypt movie, I regret wasting my time on it\",\n",
    "    \"The actors did a great job but the story lacked depth\",\n",
    "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
    "    \"This film was just okay, not too bad but not great either\",\n",
    "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
    "    \"The movie was disappointing, it did not live up to the hype\"\n",
    "]\n",
    "paragraph = ' '.join(text_data)\n",
    "\n",
    "paragraph = paragraph.lower()\n",
    "\n",
    "words = word_tokenize(paragraph)\n",
    "print(\"words len : \",len(words))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"lemmatized_words len : \",len(lemmatized_words))\n",
    "\n",
    "processed_paragraph = ' '.join(lemmatized_words)\n",
    "\n",
    "print(processed_paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fb2d7",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c77d281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['The', 'movie', 'was', 'fantastic', 'and', 'I', 'loved', 'every', 'part', 'of', 'it', 'about', 'Egypt', 'I', 'hated', 'the', 'film', ',', 'it', 'was', 'the', 'worst', 'I', 'have', 'ever', 'seen', 'The', 'storyline', 'was', 'boring', 'but', 'the', 'acting', 'was', 'brilliant', 'An', 'amazing', 'movie', 'with', 'a', 'great', 'plot', 'and', 'incredible', 'performances', 'Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it', 'The', 'actors', 'did', 'a', 'great', 'job', 'but', 'the', 'story', 'lacked', 'depth', 'One', 'of', 'the', 'best', 'films', 'I', 'have', 'seen', 'in', 'a', 'long', 'time', ',', 'highly', 'recommend', 'it', 'This', 'film', 'was', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'great', 'either', 'Absolutely', 'loved', 'the', 'movie', ',', 'fantastic', 'plot', 'and', 'wonderful', 'cast', 'The', 'movie', 'was', 'disappointing', ',', 'it', 'did', 'not', 'live', 'up', 'to', 'the', 'hype']\n",
      "Porter Stemmer: ['the', 'movi', 'wa', 'fantast', 'and', 'i', 'love', 'everi', 'part', 'of', 'it', 'about', 'egypt', 'i', 'hate', 'the', 'film', ',', 'it', 'wa', 'the', 'worst', 'i', 'have', 'ever', 'seen', 'the', 'storylin', 'wa', 'bore', 'but', 'the', 'act', 'wa', 'brilliant', 'an', 'amaz', 'movi', 'with', 'a', 'great', 'plot', 'and', 'incred', 'perform', 'egypt', 'movi', ',', 'i', 'regret', 'wast', 'my', 'time', 'on', 'it', 'the', 'actor', 'did', 'a', 'great', 'job', 'but', 'the', 'stori', 'lack', 'depth', 'one', 'of', 'the', 'best', 'film', 'i', 'have', 'seen', 'in', 'a', 'long', 'time', ',', 'highli', 'recommend', 'it', 'thi', 'film', 'wa', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'great', 'either', 'absolut', 'love', 'the', 'movi', ',', 'fantast', 'plot', 'and', 'wonder', 'cast', 'the', 'movi', 'wa', 'disappoint', ',', 'it', 'did', 'not', 'live', 'up', 'to', 'the', 'hype']\n",
      "Snowball Stemmer: ['the', 'movi', 'was', 'fantast', 'and', 'i', 'love', 'everi', 'part', 'of', 'it', 'about', 'egypt', 'i', 'hate', 'the', 'film', ',', 'it', 'was', 'the', 'worst', 'i', 'have', 'ever', 'seen', 'the', 'storylin', 'was', 'bore', 'but', 'the', 'act', 'was', 'brilliant', 'an', 'amaz', 'movi', 'with', 'a', 'great', 'plot', 'and', 'incred', 'perform', 'egypt', 'movi', ',', 'i', 'regret', 'wast', 'my', 'time', 'on', 'it', 'the', 'actor', 'did', 'a', 'great', 'job', 'but', 'the', 'stori', 'lack', 'depth', 'one', 'of', 'the', 'best', 'film', 'i', 'have', 'seen', 'in', 'a', 'long', 'time', ',', 'high', 'recommend', 'it', 'this', 'film', 'was', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'great', 'either', 'absolut', 'love', 'the', 'movi', ',', 'fantast', 'plot', 'and', 'wonder', 'cast', 'the', 'movi', 'was', 'disappoint', ',', 'it', 'did', 'not', 'live', 'up', 'to', 'the', 'hype']\n",
      "Lancaster Stemmer: ['the', 'movy', 'was', 'fantast', 'and', 'i', 'lov', 'every', 'part', 'of', 'it', 'about', 'egypt', 'i', 'hat', 'the', 'film', ',', 'it', 'was', 'the', 'worst', 'i', 'hav', 'ev', 'seen', 'the', 'storylin', 'was', 'bor', 'but', 'the', 'act', 'was', 'bril', 'an', 'amaz', 'movy', 'with', 'a', 'gre', 'plot', 'and', 'incred', 'perform', 'egypt', 'movy', ',', 'i', 'regret', 'wast', 'my', 'tim', 'on', 'it', 'the', 'act', 'did', 'a', 'gre', 'job', 'but', 'the', 'story', 'lack', 'dep', 'on', 'of', 'the', 'best', 'film', 'i', 'hav', 'seen', 'in', 'a', 'long', 'tim', ',', 'high', 'recommend', 'it', 'thi', 'film', 'was', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'gre', 'eith', 'absolv', 'lov', 'the', 'movy', ',', 'fantast', 'plot', 'and', 'wond', 'cast', 'the', 'movy', 'was', 'disappoint', ',', 'it', 'did', 'not', 'liv', 'up', 'to', 'the', 'hyp']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "### Snowball Stemmer\n",
    "# It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "porter_stems = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "snowball_stems = [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "lancaster_stems = [lancaster_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Porter Stemmer:\", porter_stems)\n",
    "print(\"Snowball Stemmer:\", snowball_stems)\n",
    "print(\"Lancaster Stemmer:\", lancaster_stems)\n",
    "# congratulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae6aaa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The: The\n",
      "movie: movie\n",
      "was: wa\n",
      "fantastic: fantastic\n",
      "and: and\n",
      "I: I\n",
      "loved: loved\n",
      "every: every\n",
      "part: part\n",
      "of: of\n",
      "it: it\n",
      "about: about\n",
      "Egypt: Egypt\n",
      "I: I\n",
      "hated: hated\n",
      "the: the\n",
      "film: film\n",
      ",: ,\n",
      "it: it\n",
      "was: wa\n",
      "the: the\n",
      "worst: worst\n",
      "I: I\n",
      "have: have\n",
      "ever: ever\n",
      "seen: seen\n",
      "The: The\n",
      "storyline: storyline\n",
      "was: wa\n",
      "boring: boring\n",
      "but: but\n",
      "the: the\n",
      "acting: acting\n",
      "was: wa\n",
      "brilliant: brilliant\n",
      "An: An\n",
      "amazing: amazing\n",
      "movie: movie\n",
      "with: with\n",
      "a: a\n",
      "great: great\n",
      "plot: plot\n",
      "and: and\n",
      "incredible: incredible\n",
      "performances: performance\n",
      "Egypt: Egypt\n",
      "movie: movie\n",
      ",: ,\n",
      "I: I\n",
      "regret: regret\n",
      "wasting: wasting\n",
      "my: my\n",
      "time: time\n",
      "on: on\n",
      "it: it\n",
      "The: The\n",
      "actors: actor\n",
      "did: did\n",
      "a: a\n",
      "great: great\n",
      "job: job\n",
      "but: but\n",
      "the: the\n",
      "story: story\n",
      "lacked: lacked\n",
      "depth: depth\n",
      "One: One\n",
      "of: of\n",
      "the: the\n",
      "best: best\n",
      "films: film\n",
      "I: I\n",
      "have: have\n",
      "seen: seen\n",
      "in: in\n",
      "a: a\n",
      "long: long\n",
      "time: time\n",
      ",: ,\n",
      "highly: highly\n",
      "recommend: recommend\n",
      "it: it\n",
      "This: This\n",
      "film: film\n",
      "was: wa\n",
      "just: just\n",
      "okay: okay\n",
      ",: ,\n",
      "not: not\n",
      "too: too\n",
      "bad: bad\n",
      "but: but\n",
      "not: not\n",
      "great: great\n",
      "either: either\n",
      "Absolutely: Absolutely\n",
      "loved: loved\n",
      "the: the\n",
      "movie: movie\n",
      ",: ,\n",
      "fantastic: fantastic\n",
      "plot: plot\n",
      "and: and\n",
      "wonderful: wonderful\n",
      "cast: cast\n",
      "The: The\n",
      "movie: movie\n",
      "was: wa\n",
      "disappointing: disappointing\n",
      ",: ,\n",
      "it: it\n",
      "did: did\n",
      "not: not\n",
      "live: live\n",
      "up: up\n",
      "to: to\n",
      "the: the\n",
      "hype: hype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Q&A,chatbots,text summarization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word}: {lemmatizer.lemmatize(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db5410ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet');\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize(\"better\", pos=\"a\"))   # → good\n",
    "print(lemma.lemmatize(\"was\", pos=\"v\"))      # → be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c555456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better as Nono:  better\n",
      "better as adjective:  good\n",
      "was as Nono:  wa\n",
      "was as verb:  be\n"
     ]
    }
   ],
   "source": [
    "#\"n\" → noun\n",
    "# \"v\" → verb\n",
    "# \"a\" → adjective\n",
    "# \"r\" → adverb\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(\"better as Nono: \",lemma.lemmatize(\"better\"))             # بدون pos → better (اعتبرها noun)\n",
    "print(\"better as adjective: \",lemma.lemmatize(\"better\", pos=\"a\"))    # كصفة adjective → good\n",
    "\n",
    "print(\"was as Nono: \",lemma.lemmatize(\"was\"))                # بدون pos → was (ما قدر يرجع للجذر)\n",
    "print(\"was as verb: \",lemma.lemmatize(\"was\", pos=\"v\"))       # كفعل verb → be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0b1cd",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f09c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The || DET || determiner\n",
      "movie || NOUN || noun\n",
      "was || AUX || auxiliary\n",
      "fantastic || ADJ || adjective\n",
      "and || CCONJ || coordinating conjunction\n",
      "I || PRON || pronoun\n",
      "loved || VERB || verb\n",
      "every || DET || determiner\n",
      "part || NOUN || noun\n",
      "of || ADP || adposition\n",
      "it || PRON || pronoun\n",
      "about || ADP || adposition\n",
      "Egypt || PROPN || proper noun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_data[0])\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\"||\", token.pos_,\"||\", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c766340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "hated  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
      "the  |  DET  |  determiner  |  DT  |  determiner\n",
      "film  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n",
      ",  |  PUNCT  |  punctuation  |  ,  |  punctuation mark, comma\n",
      "it  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "was  |  AUX  |  auxiliary  |  VBD  |  verb, past tense\n",
      "the  |  DET  |  determiner  |  DT  |  determiner\n",
      "worst  |  ADJ  |  adjective  |  JJS  |  adjective, superlative\n",
      "I  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "have  |  AUX  |  auxiliary  |  VBP  |  verb, non-3rd person singular present\n",
      "ever  |  ADV  |  adverb  |  RB  |  adverb\n",
      "seen  |  VERB  |  verb  |  VBN  |  verb, past participle\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text_data[1])\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a652dac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"53e81a43b12b447bbeb7c5385e01f9f2-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">hated</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">film,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">worst</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">have</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">ever</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">seen</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 920.0,89.5 920.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,354.0 L573.0,342.0 557.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,354.0 L1273.0,342.0 1257.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,89.5 1970.0,89.5 1970.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-8\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-9\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-10\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,2.0 1975.0,2.0 1975.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53e81a43b12b447bbeb7c5385e01f9f2-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1975.0,354.0 L1983.0,342.0 1967.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"dep\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280b6e0",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2208f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b92bb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "egypt  |  GPE  |  Countries, cities, states\n",
      "egypt  |  GPE  |  Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(paragraph)\n",
    "for ent in doc1.ents:\n",
    "    print(ent ,\" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a330aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the movie was fantastic and i loved every part of it about \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    egypt\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " i hated the film, it was the worst i have ever seen the storyline was boring but the acting was brilliant an amazing movie with a great plot and incredible performances \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    egypt\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " movie, i regret wasting my time on it the actors did a great job but the story lacked depth one of the best films i have seen in a long time, highly recommend it this film was just okay, not too bad but not great either absolutely loved the movie, fantastic plot and wonderful cast the movie was disappointing, it did not live up to the hype</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc1, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a11275df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved as NLP_task_results.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure resources are downloaded\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "\n",
    "\n",
    "# Tools\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, sentence in enumerate(text_data, start=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens_no_stop = [w for w in tokens if w.lower() not in stop_words]\n",
    "    stems = [stemmer.stem(w) for w in tokens_no_stop]\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in tokens_no_stop]\n",
    "    pos_tags = nltk.pos_tag(tokens_no_stop)\n",
    "    \n",
    "    results.append({\n",
    "        \"Sentence_ID\": idx,\n",
    "        \"Original_Sentence\": sentence,\n",
    "        \"Tokens\": tokens,\n",
    "        \"Tokens_No_Stop\": tokens_no_stop,\n",
    "        \"Stems\": stems,\n",
    "        \"Lemmas\": lemmas,\n",
    "        \"POS_Tags\": pos_tags\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"NLP_task_results.csv\", index=False)\n",
    "print(\"✅ Saved as NLP_task_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1971df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ghane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")  # for newer NLTK versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aebf60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
